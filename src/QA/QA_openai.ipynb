{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "from langchain.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.chains import ChatVectorDBChain, ConversationalRetrievalChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.chat_vector_db.prompts import (CONDENSE_QUESTION_PROMPT,\n",
    "                                                     QA_PROMPT)\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhtu/Desktop/perso/langchain-examples/.venv/lib/python3.9/site-packages/langchain/document_loaders/readthedocs.py:48: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 48 of the file /Users/thanhtu/Desktop/perso/langchain-examples/.venv/lib/python3.9/site-packages/langchain/document_loaders/readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  _ = BeautifulSoup(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m         pickle\u001b[39m.\u001b[39mdump(vectorstore, f)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m vectorstore\n\u001b[0;32m---> 20\u001b[0m vectorstore \u001b[39m=\u001b[39m ingest_document()\n\u001b[1;32m     21\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     22\u001b[0m memory \u001b[39m=\u001b[39m ConversationBufferMemory(memory_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m, return_messages\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mingest_document\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m documents \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39msplit_documents(raw_documents)\n\u001b[1;32m     10\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYOUR_API_KEY\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m vectorstore \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(documents, embeddings)\n\u001b[1;32m     13\u001b[0m \u001b[39m#save vectorstore\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvectorstore.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/perso/langchain-examples/.venv/lib/python3.9/site-packages/langchain/vectorstores/base.py:317\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    316\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/perso/langchain-examples/.venv/lib/python3.9/site-packages/langchain/vectorstores/faiss.py:514\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \n\u001b[1;32m    498\u001b[0m \u001b[39mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__from(\n\u001b[1;32m    515\u001b[0m     texts,\n\u001b[1;32m    516\u001b[0m     embeddings,\n\u001b[1;32m    517\u001b[0m     embedding,\n\u001b[1;32m    518\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[1;32m    519\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    520\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    521\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/perso/langchain-examples/.venv/lib/python3.9/site-packages/langchain/vectorstores/faiss.py:465\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__from\u001b[39m(\n\u001b[1;32m    455\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    463\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[1;32m    464\u001b[0m     faiss \u001b[39m=\u001b[39m dependable_faiss_import()\n\u001b[0;32m--> 465\u001b[0m     index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mIndexFlatL2(\u001b[39mlen\u001b[39m(embeddings[\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m    466\u001b[0m     vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(embeddings, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    467\u001b[0m     \u001b[39mif\u001b[39;00m normalize_L2:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "def ingest_document() -> VectorStore: \n",
    "    \"\"\"Load documents from web pages\n",
    "    This step allows to create embeddings from raw text, splitted by chunk. \n",
    "    The embeddings are stored in a vectorstore database and can be used for further processing.\n",
    "    \"\"\"\n",
    "    loader = ReadTheDocsLoader(\"langchain.readthedocs.io/en/latest/\")\n",
    "    raw_documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=\"YOUR_API_KEY\") # type: ignore\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    #save vectorstore\n",
    "    with open(\"vectorstore.pickle\", \"wb\") as f:\n",
    "        pickle.dump(vectorstore, f)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "vectorstore = ingest_document()\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", verbose=True)  # type: ignore\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "chat_qa = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                        retriever=vectorstore.as_retriever(), \n",
    "                                        condense_question_prompt=CONDENSE_QUESTION_PROMPT, \n",
    "                                        qa_prompt=QA_PROMPT, \n",
    "                                        #return_source_documents=True,\n",
    "                                        memory=memory,\n",
    "                                        get_chat_history = lambda x: x, \n",
    "                                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_qa\u001b[39m.\u001b[39mrun(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHow you can help me?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_qa' is not defined"
     ]
    }
   ],
   "source": [
    "chat_qa.run(input=\"How you can help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=500)\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "    \n",
    "    def bot(history):\n",
    "        print(history)\n",
    "        bot_message = chat_qa.run({\"question\": history[-1][0], \"chat_history\" : history[:-1]})\n",
    "        history[-1][1] = bot_message\n",
    "        return history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
