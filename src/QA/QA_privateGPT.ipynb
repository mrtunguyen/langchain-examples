{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import glob\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if 'text/html content not found in email' in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc\n",
    "    \n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    # \".docx\": (Docx2txtLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyMuPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
    "        )\n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_documents(source_directory, chunk_size : int=1000, chunk_overlap: int = 200, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts\n",
    "\n",
    "def check_vectorstore_exist(persist_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
    "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
    "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
    "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
    "            # At least 3 documents are needed in a working vectorstore\n",
    "            if len(list_index_files) > 3:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def create_vectorstore(persist_directory: str, source_directory: str, chroma_settings: Settings): \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\", device=\"cpu\")\n",
    "\n",
    "    if check_vectorstore_exist(persist_directory):\n",
    "        print(f\"Vectorstore already exists in {persist_directory}\")\n",
    "        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=chroma_settings)\n",
    "        collection = db.get()\n",
    "        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
    "        print(f\"Creating embeddings. May take some minutes...\")\n",
    "        db.add_documents(texts)\n",
    "    else: \n",
    "        texts = process_documents(source_directory)\n",
    "        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=chroma_settings)\n",
    "    db.persist()\n",
    "    db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIRECTORY=\"db\"\n",
    "MODEL_TYPE=\"GPT4All\"\n",
    "MODEL_PATH=\"models/ggml-gpt4all-j-v1.3-groovy.bin\"\n",
    "EMBEDDINGS_MODEL_NAME=\"all-MiniLM-L6-v2\"\n",
    "MODEL_N_CTX=1000\n",
    "MODEL_N_BATCH=8\n",
    "TARGET_SOURCE_CHUNKS=4\n",
    "\n",
    "CHROMA_SETTINGS = Settings(\n",
    "        chroma_db_impl='duckdb+parquet',\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "create_vectorstore(PERSIST_DIRECTORY, \"source_documents\", CHROMA_SETTINGS)\n",
    "\n",
    "db = Chroma(persist_directory=PERSIST_DIRECTORY, client_settings=CHROMA_SETTINGS)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler]\n",
    "llm = LlamaCpp(model_path=MODEL_PATH, n_ctx=MODEL_N_CTX, n_batch=MODEL_N_BATCH, callbacks=callbacks, verbose=False) # type: ignore\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents= True)\n",
    "while True:\n",
    "    query = input(\"\\nEnter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Get the answer from the chain\n",
    "    start = time.time()\n",
    "    res = qa(query)\n",
    "    answer, docs = res['result'], res['source_documents']\n",
    "    end = time.time()\n",
    "\n",
    "    # Print the result\n",
    "    print(\"\\n\\n> Question:\")\n",
    "    print(query)\n",
    "    print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n",
    "    print(answer)\n",
    "\n",
    "    # Print the relevant sources used for the answer\n",
    "    for document in docs:\n",
    "        print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
    "        print(document.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
